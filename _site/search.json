[
  {
    "objectID": "Handson_Ex/Handson_Ex03/Handson_Ex03.html",
    "href": "Handson_Ex/Handson_Ex03/Handson_Ex03.html",
    "title": "Handson_Ex03",
    "section": "",
    "text": "Load packages and data:\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)\nexam_data <- read_csv(\"data/Exam_data.csv\")\n\n\n\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\nThe first three lines of codes in the code chunk create a new field called *tooltip*. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\n\nexam_data$tooltip <- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nCustomize toolpit:\n\ntooltip_css <- \"background-color:white; #<<\nfont-style:bold; color:black;\" #<<\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #<<\n    opts_tooltip(    #<<\n      css = tooltip_css)) #<<\n)    \n\n\n\n\n\nIn this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip <- function(y, ymax, accuracy = .01) {\n  mean <- scales::number(y, accuracy = accuracy)\n  sem <- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point <- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n) \n\n\n\n\n\nHighlighting effect:\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n) \n\n\n\n\n\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)  \n\n\n\n\n\n`onclick` argument of ggiraph provides hotlink interactivity on the web.\n\nexam_data$onclick <- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np <- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)     \n\n\n\n\n\n\n\n\n1. Appropriate interactive functions of **ggiraph** will be used to create the multiple views.\n2. *patchwork* function of [patchwork](https://patchwork.data-imaginist.com/) package will be used inside girafe function to create the interactive coordinated multiple views.\n\np1 <- ggplot(data=exam_data, \n           aes(x = MATHS)) +\n      geom_dotplot_interactive(              \n        aes(data_id = ID),              \n        stackgroups = TRUE,                  \n        binwidth = 1,                        \n        method = \"histodot\") +  \n      coord_cartesian(xlim=c(0,100)) + \n      scale_y_continuous(NULL,               \n                         breaks = NULL)\n\np2 <- ggplot(data=exam_data, \n           aes(x = ENGLISH)) +\n      geom_dotplot_interactive(              \n        aes(data_id = ID),              \n        stackgroups = TRUE,                  \n        binwidth = 1,                        \n        method = \"histodot\") + \n      coord_cartesian(xlim=c(0,100)) + \n      scale_y_continuous(NULL,               \n                         breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n           width_svg = 6,\n           height_svg = 3,\n           options = list(\n             opts_hover(css = \"fill: #202020;\"),\n             opts_hover_inv(css = \"opacity:0.2;\")\n             )\n           ) \n\n\n\n\n\nCreating an interactive scatter plot: plot_ly() method\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\nCreating an interactive scatter plot: ggplotly() method\n\np <- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\n- [`highlight_key()`](https://www.rdocumentation.org/packages/plotly/versions/4.9.2/topics/highlight_key) of **plotly** package is used as shared data.\n- two scatterplots will be created by using ggplot2 functions.\n- lastly, [*subplot()*](https://plotly.com/r/subplots/) of **plotly** package is used to place them next to each other side-by-side.\n\nd <- highlight_key(exam_data)\np1 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 <- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\nInteractive Data Table: DT package\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\nd <- highlight_key(exam_data) \np <- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 6)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#animated-data-visualisation-gganimate-methods",
    "href": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#animated-data-visualisation-gganimate-methods",
    "title": "Handson_Ex03",
    "section": "Animated Data Visualisation: gganimate methods",
    "text": "Animated Data Visualisation: gganimate methods\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#animated-data-visualization-plotly",
    "href": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#animated-data-visualization-plotly",
    "title": "Handson_Ex03",
    "section": "Animated Data Visualization: plotly",
    "text": "Animated Data Visualization: plotly\n\ngg <- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\nbp <- globalPop %>%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent, \n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          )\nbp"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html",
    "title": "Handson_Ex04",
    "section": "",
    "text": "pacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n\nexam <-read.csv(\"data/Exam_data.csv\")\n\n\n\nType of statistic expected (\"parametric\" or \"nonparametric\" or \"robust\" or \"bayes\").Corresponding abbreviations are also accepted: \"p\" (for parametric), \"np\" (nonparametric), \"r\" (robust), or \"bf\"resp.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\nexam1 <- exam %>% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n\n\n\n\n\npacman::p_load(readxl, performance, parameters, see)          \n\n\n\n\ncar_resale <- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale       \n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   <dbl> <chr>    <dbl>     <dbl>     <dbl>    <dbl>  <dbl>         <dbl>  <dbl>\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period <dbl>, HP_Bin <chr>, CC_bin <chr>,\n#   Doors <dbl>, Gears <dbl>, Cylinders <dbl>, Fuel_Type <chr>, Color <chr>,\n#   Met_Color <dbl>, Automatic <dbl>, Mfr_Guarantee <dbl>,\n#   BOVAG_Guarantee <dbl>, ABS <dbl>, Airbag_1 <dbl>, Airbag_2 <dbl>,\n#   Airco <dbl>, Automatic_airco <dbl>, Boardcomputer <dbl>, CD_Player <dbl>,\n#   Central_Lock <dbl>, Powered_Windows <dbl>, Power_Steering <dbl>, …\n\n\n\n\n\n\nmodel <- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c <- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\nmodel1 <- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n <- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\n\ncheck_h <- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#visualizing-the-uncertainty-of-point-estimates",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#visualizing-the-uncertainty-of-point-estimates",
    "title": "Handson_Ex04",
    "section": "Visualizing the uncertainty of point estimates",
    "text": "Visualizing the uncertainty of point estimates\nImport packages and data\n\npacman::p_load(tidyverse, plotly, crosstalk, DT, ggdist, gganimate)\nexam <- read_csv(\"data/Exam_data.csv\")\n\n\nVisualizing the uncertainty of point estimates: ggplot2 methods\n\nmy_sum <- exam %>%\n  group_by(RACE) %>%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %>%\n  mutate(se=sd/sqrt(n-1))\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n \n  \n    RACE \n    n \n    mean \n    sd \n    se \n  \n \n\n  \n    Chinese \n    193 \n    76.50777 \n    15.69040 \n    1.132357 \n  \n  \n    Indian \n    12 \n    60.66667 \n    23.35237 \n    7.041005 \n  \n  \n    Malay \n    108 \n    57.44444 \n    21.13478 \n    2.043177 \n  \n  \n    Others \n    9 \n    69.66667 \n    10.72381 \n    3.791438 \n  \n\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates: ggplot2 methods\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean \n          maths score by race\")\n\n\n\n\nPlot the 95% confidence interval of mean maths score by race. The error bars should be sorted by the average maths scores.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-qt(0.025,df=n,lower.tail = FALSE)*se, \n        ymax=mean+qt(0.025,df=n,lower.tail = FALSE)*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=reorder(RACE, -mean), \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates with interactive error bars\n\nd <- highlight_key(my_sum) \np <- ggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-qt(0.01,df=n,lower.tail = FALSE)*se, \n        ymax=mean+qt(0.01,df=n,lower.tail = FALSE)*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=reorder(RACE, -mean), \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"99% confidence interval of mean maths score by race\")\n\ngg <- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 6)        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\n\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\nMakeover the plot on previous slide by showing 95% and 99% confidence intervals.\n\nexam %>%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.99,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\n\nexam %>%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#visualizing-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#visualizing-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Handson_Ex04",
    "section": "Visualizing Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "Visualizing Uncertainty with Hypothetical Outcome Plots (HOPs)\nImport packages\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Handson_Ex04",
    "section": "Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "Funnel Plot for Fair Visual Comparison: ggplot2 methods\n\nComputing the basic derived field\n\ndf <- covid19 %>%\n  mutate(rate = Death / Positive) %>%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %>%\n  filter(rate > 0)\n\nfit.mean <- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\nCalculate lower and upper limits for 95% and 99.9% CI\n\nnumber.seq <- seq(1, max(df$Positive), 1)\nnumber.ll95 <- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 <- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 <- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 <- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI <- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\nPlotting a static funnel plot\n\np <- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\nInteractive Funnel Plot: plotly + ggplot2\n\nfp_ggplotly <- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex05/Handson_Ex05.html",
    "href": "Handson_Ex/Handson_Ex05/Handson_Ex05.html",
    "title": "Handson_Ex05",
    "section": "",
    "text": "Show the code\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\n\n\nShow the code\nGAStech_nodes <- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges <- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\n\n\n\nShow the code\nglimpse(GAStech_edges)\n\n\nRows: 9,063\nColumns: 8\n$ source      <dbl> 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      <dbl> 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    <chr> \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    <time> 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     <chr> \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject <chr> \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel <chr> \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel <chr> \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n\n\n\nShow the code\nGAStech_edges <- GAStech_edges %>%\n  mutate(SendDate = dmy(SentDate)) %>%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\n\nboth dmy() and wday() are functions of lubridate package. lubridate is an R package that makes it easier to work with dates and times.\ndmy() transforms the SentDate to Date data type.\nwday() returns the day of the week as a decimal number or an ordered factor if label is TRUE. The argument abbr is FALSE keep the daya spells in full, i.e. Monday. The function will create a new column in the data.frame i.e. Weekday and the output of wday() will save in this newly created field.\nthe values in the Weekday field are in ordinal scale.\n\n\n\n\n\n\n\n\nShow the code\nGAStech_edges_aggregated <- GAStech_edges %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(source, target, Weekday) %>%\n    summarise(Weight = n()) %>%\n  filter(source!=target) %>%\n  filter(Weight > 1) %>%\n  ungroup()\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nfour functions from dplyr package are used. They are: filter(), group(), summarise(), and ungroup().\nThe output data.frame is called GAStech_edges_aggregated.\nA new field called Weight has been added in GAStech_edges_aggregated.\n\n\n\n\n\n\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\n\n\nShow the code\nGAStech_graph <- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\nShow the code\nGAStech_graph\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 54 × 4\n     id label               Department     Title                                \n  <dbl> <chr>               <chr>          <chr>                                \n1     1 Mat.Bramar          Administration Assistant to CEO                     \n2     2 Anda.Ribera         Administration Assistant to CFO                     \n3     3 Rachel.Pantanal     Administration Assistant to CIO                     \n4     4 Linda.Lagos         Administration Assistant to COO                     \n5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Manag…\n6     6 Carla.Forluniau     Administration Assistant to IT Group Manager        \n# ℹ 48 more rows\n#\n# A tibble: 1,372 × 4\n   from    to Weekday Weight\n  <int> <int> <ord>    <int>\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\n\n\nShow the code\nGAStech_graph %>%\n  activate(edges) %>%\n  arrange(desc(Weight))\n\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# A tibble: 1,372 × 4\n   from    to Weekday  Weight\n  <int> <int> <ord>     <int>\n1    40    41 Saturday     13\n2    41    43 Monday       11\n3    35    31 Tuesday      10\n4    40    41 Monday       10\n5    40    43 Monday       10\n6    36    32 Sunday        9\n# ℹ 1,366 more rows\n#\n# A tibble: 54 × 4\n     id label           Department     Title           \n  <dbl> <chr>           <chr>          <chr>           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\n\n\n\n\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\n\n\n\n\nShow the code\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\n\nShow the code\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nShow the code\ng <- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\n\n\nShow the code\ng <- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nShow the code\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nShow the code\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\n\n\nShow the code\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)       \n\n\n\n\n\n\n\n\n\n\nShow the code\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)      \n\n\n\n\n\n\n\n\n\n\nShow the code\nset_graph_style() \n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nShow the code\nset_graph_style()\n\ng <- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\n\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\n\nShow the code\ng <- GAStech_graph %>%\n  mutate(betweenness_centrality = centrality_betweenness()) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nShow the code\ng <- GAStech_graph %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\n\nShow the code\ng <- GAStech_graph %>%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %>%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\n\n\nShow the code\nGAStech_edges_aggregated <- GAStech_edges %>%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %>%\n  rename(from = id) %>%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %>%\n  rename(to = id) %>%\n  filter(MainSubject == \"Work related\") %>%\n  group_by(from, to) %>%\n    summarise(weight = n()) %>%\n  filter(from!=to) %>%\n  filter(weight > 1) %>%\n  ungroup()\n\n\n\n\n\n\n\nShow the code\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\n\n\n\n\n\nShow the code\nGAStech_nodes <- GAStech_nodes %>%\n  rename(group = Department) \n\n\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex07/Handson_Ex07.html",
    "href": "Handson_Ex/Handson_Ex07/Handson_Ex07.html",
    "title": "Handson_Ex07",
    "section": "",
    "text": "Show the code\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\n\nImport eventlog.csv file into R environment and called the data frame as attacks.\n\n\nShow the code\n#Reading the data into R environment\nattacks <- read_csv(\"data/eventlog.csv\")\n\n\n\n\n\n\n\nShow the code\nkable(head(attacks))\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\nStep 1: Deriving weekday and hour of day fields\n\n\nShow the code\nmake_hr_wkday <- function(ts, sc, tz) {\n  real_times <- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt <- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\n\nShow the code\nwkday_levels <- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks <- attacks %>%\n  group_by(tz) %>%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %>% \n  ungroup() %>% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\n\nNotes\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they'll be ordered when plotting\n\n\nFinal check:\n\n\nShow the code\nkable(head(attacks))\n\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n\n\nShow the code\ngrouped <- attacks %>% \n  count(wkday, hour) %>% \n  ungroup() %>%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\n\n\n\n\nShow the code\n#plot a cluster heatmap\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\n\nShow the code\nattacks_by_country <- count(\n  attacks, source_country) %>%\n  mutate(percent = percent(n/sum(n))) %>%\n  arrange(desc(n))\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\n\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\n\nShow the code\ntop4 <- attacks_by_country$source_country[1:4]\ntop4_attacks <- attacks %>%\n  filter(source_country %in% top4) %>%\n  count(source_country, wkday, hour) %>%\n  ungroup() %>%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %>%\n  na.omit()    \n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\n\nShow the code\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )  \n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nair <- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\nDeriving month and year fields\n\n\nShow the code\nair$month <- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year <- year(ymd(air$`Month-Year`))\n\n\nExtracting the target country\n\n\nShow the code\nVietnam <- air %>% \n  select(`Vietnam`, \n         month, \n         year) %>%\n  filter(year >= 2010)\n\n\nComputing year average arrivals by month\n\n\nShow the code\nhline.data <- Vietnam %>% \n  group_by(month) %>%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\n\n\n\nShow the code\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\")\n\n\n\n\n\n\n\n\n\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n\n\n\nShow the code\nrice <- read_csv(\"data/rice.csv\")\n\n\n\n\n\n\n\nShow the code\nrice %>% \n  mutate(Year = factor(Year)) %>%\n  filter(Year %in% c(1961, 1980)) %>%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor.\n\n\n\n\n\n\nLoading packages:\n\n\nShow the code\npacman::p_load(ggHoriPlot, ggthemes, tidyverse)\n\n\n\n\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\n\n\nShow the code\naverp <- read_csv(\"data/AVERP.csv\") %>%\n  mutate(`Date` = dmy(`Date`))\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nBy default, read_csv will import data in Date field as Character data type. dmy() of lubridate package to palse the Date field into appropriate Date data type in R.\n\n\n\n\n\n\n\n\nShow the code\naverp %>% \n  filter(Date >= \"2018-01-01\") %>%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html",
    "title": "Handson_Ex01",
    "section": "",
    "text": "pacman::p_load(tidyverse)\n\n\n\n\n\nexam_data <-read.csv(\"data/Exam_data.csv\")\n\n\n\n\n\nhist(exam_data$MATHS)\n\n\n\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"grey\") +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"black\",      \n                 fill=\"light blue\")  \n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()\n\n\n\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point()            \n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex06/Handson_Ex06.html",
    "href": "Handson_Ex/Handson_Ex06/Handson_Ex06.html",
    "title": "Handson_Ex06",
    "section": "",
    "text": "Show the code\npacman::p_load('plotly', 'tidyverse')\n\n\n\n\nShow the code\n#require(devtools)\n#install_version(\"ggtern\", version = \"3.4.1\")\n\n\n\n\nShow the code\n#library(ggtern)\n\n\n\n\n\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\nShow the code\n#Reading the data into R environment\npop_data <- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\n\n\n\nShow the code\n#Deriving the young, economy active and old measures\nagpop_mutated <- pop_data %>%\n  mutate(`Year` = as.character(Year))%>%\n  spread(AG, Population) %>%\n  mutate(YOUNG = rowSums(.[4:8]))%>%\n  mutate(ACTIVE = rowSums(.[9:16]))  %>%\n  mutate(OLD = rowSums(.[17:21])) %>%\n  mutate(TOTAL = rowSums(.[22:24])) %>%\n  filter(Year == 2018)%>%\n  filter(TOTAL > 0)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output report of GAStech_edges above reveals that the SentDate is treated as “Character” data type instead of date data type. This is an error! Before we continue, it is important for us to change the data type of SentDate field back to “Date”” data type.\n\n\n\n\n\n\n\n\n\nShow the code\n#Building the static ternary plot\n#ggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n#  geom_point()\n\n\n\n\nShow the code\n#Building the static ternary plot\n#ggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n#  geom_point() +\n#  labs(title=\"Population structure, 2015\") +\n#  theme_rgbw()\n\n\n\n\n\n\n\nShow the code\n# reusable function for creating annotation object\nlabel <- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis <- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes <- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %>%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex06/Handson_Ex06.html#visual-multivariate-analysis-with-parallel-coordinates-plot",
    "href": "Handson_Ex/Handson_Ex06/Handson_Ex06.html#visual-multivariate-analysis-with-parallel-coordinates-plot",
    "title": "Handson_Ex06",
    "section": "Visual Multivariate Analysis with Parallel Coordinates Plot",
    "text": "Visual Multivariate Analysis with Parallel Coordinates Plot\nParallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), \"This certainly isn't a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn't in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.\" For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package.\n\n\nPackages\n\n\nShow the code\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\nData preparation\n\n\nShow the code\nwh <- read_csv(\"data/WHData-2018.csv\")\n\n\n\n\nPlotting Static Parallel Coordinates Plot\nPlotting a simple parallel coordinates\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\nPlotting a parallel coordinates with boxplot\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\nThings to learn from the code chunk above.\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\nParallel coordinates with facet\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\nRotating x-axis text label\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\nAdjusting the rotated x-axis text label\n\n\nShow the code\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\nPlotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using 'htmlwidgets' package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\nThe basic plot\n\n\nShow the code\nwh <- wh %>%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\nRotate axis label\n\n\nShow the code\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\nChanging the colour scheme\n\n\nShow the code\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\nParallel coordinates plot with histogram\n\n\nShow the code\nhistoVisibility <- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex06/Handson_Ex06.html#treemap-visualisation-with-r",
    "href": "Handson_Ex/Handson_Ex06/Handson_Ex06.html#treemap-visualisation-with-r",
    "title": "Handson_Ex06",
    "section": "Treemap Visualisation with R",
    "text": "Treemap Visualisation with R\n\nPackages\n\n\nShow the code\npacman::p_load(treemap, treemapify, tidyverse) \n\n\n\n\nData wrangling\n\n\nShow the code\nrealis2018 <- read_csv(\"data/realis2018.csv\")\n\n\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they'll be automatically applied \"by group\".\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(\"window-functions\").\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\n\n\nShow the code\nrealis2018_grouped <- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised <- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\nShow the code\nrealis2018_summarised <- realis2018 %>% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %>%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\nDesigning Treemap with treemap Package\nDesigning a static treemap\n\n\nShow the code\nrealis2018_selected <- realis2018_summarised %>%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\nUsing basic arguments\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it's vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\nWarning:\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\nShow the code\n#Working with vColor and type arguments\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nValue type treemap\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\nManual type treemap\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nWorking with algorithm argument\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nUsing sortID\n\n\nShow the code\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nDesigning Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to \"treemapify\" its user guide.\n\n\nShow the code\n#basic treemap\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\nShow the code\n#defining hierarchy\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\nShow the code\n#Groupping by planning area\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\nShow the code\n#adding boundary line\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")\n\n\n\n\n\n\n\nDesigning Interactive Treemap using d3treeR\n\n\nShow the code\nlibrary(d3treeR)\n\n\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\n\nShow the code\ntm <- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\n\nShow the code\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex08/Handson_Ex08.html",
    "href": "Handson_Ex/Handson_Ex08/Handson_Ex08.html",
    "title": "Handson_Ex08",
    "section": "",
    "text": "Show the code\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\n\nUse the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n\nShow the code\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/SMU/Term Apr/Visual Analytics/liangyao2023/ISSS608-VAA/Handson_Ex/Handson_Ex08/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nImport respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\n\n\nShow the code\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\n\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\nStep1: Wrangling data\n\n\nShow the code\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\nStep 2: Joining the attribute data and geospatial data\n\n\nShow the code\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\n\n\n\nShow the code\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\n\nThing to learn here:\n\n\n\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\nFinal check:\n\n\nShow the code\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\nThings to learn here\n\n\n\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nStep 1: Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\nStep 2: Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")  \n\n\n\n\n\n\n\n\n\n\n\nThings to learn here:\n\n\n\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\nStep 3: Drawing a choropleth map using tm_fill() and *tm_border()*\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be us\nStep 1: Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\nClassified by “Economy Active”, in 2 classes\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"ECONOMY ACTIVE\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nOr in 10 classes:\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"ECONOMY ACTIVE\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nClassify by “Age”?\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"ECONOMY ACTIVE\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nStep 2: Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nShow the code\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nUsing ColourBrewer palette\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nMap legend: In tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nMap style: tmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"col_blind\")\n\n\n\n\n\nCartographic Furniture: Beside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\n\nShow the code\ntmap_style(\"white\")\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\nBy assigning multiple values to at least one of the aesthetic arguments:\n\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\n\n\nShow the code\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\nBy defining a group-by variable in tm_facets()\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\n\n\nShow the code\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nBy creating multiple stand-alone maps with tmap_arrange()\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\n\n\nShow the code\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\n\nShow the code\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System.\n\n\nShow the code\nsgpools <- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\n\n\n\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\n\nShow the code\nsgpools_sf <- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n\n\n\n\n\n\n\nThings to learn here:\n\n\n\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\n\n\n\n\n\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used.\nThe code churn below will turn on the interactive mode of tmap.\n\n\nShow the code\ntmap_mode(\"view\")\n\n\n\n\nThe code chunks below are used to create an interactive point symbol map.\n\n\nShow the code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\n\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\n\nShow the code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\n\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\n\nShow the code\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\n\nShow the code\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, it is wiser to switch tmap’s Viewer back to plot mode by using the code chunk below.\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\n\n\n\n\n\n\n\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\n\n\nShow the code\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")\n\n\n\n\n\n\n\n\n\nShow the code\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\n\n\nShow the code\ntmap_arrange(p2, p1, nrow = 1)\n\n\n\n\n\n\n\n\n\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\nShow the code\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\nData Preparation\n\nStep 1: Exclude records with NA by using the code chunk below.\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\n\nStep 2: Creating customised classification and extracting values\n\n\nShow the code\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\nWhy writing functions?\n\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\nCreating the get.var function\n\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\nShow the code\nget.var <- function(vname,df) {\n  v <- df[vname] %>% \n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\nA percentile mapping function\n\n\n\nShow the code\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\nTest drive the percentile mapping function\n\n\n\nShow the code\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\n\nThis is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n\n\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\n\nShow the code\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\nStep1: Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\n\nShow the code\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\nStep2: Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\nShow the code\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\nStep3: Test drive the newly created function\n\n\nShow the code\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\nStep4: Boxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\n\nShow the code\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex08/data/geospatial/MPSZ-2019.html",
    "href": "Handson_Ex/Handson_Ex08/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "This is the website for ISSS608 Visual Analytics and Applications."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home_Ex01",
    "section": "",
    "text": "City of Engagement, with a total population of 50,000, is a small city located at Country of Nowhere. The city serves as a service centre of an agriculture region surrounding the city. The main agriculture of the region is fruit farms and vineyards. The local council of the city is in the process of preparing the Local Plan 2023. A sample survey of 1000 representative residents had been conducted to collect data related to their household demographic and spending patterns, among other things. The city aims to use the data to assist with their major community revitalization efforts, including how to allocate a very large city renewal grant they have recently received.\n\n\n\nFor the purpose of this study, two data sets are provided. They are:\nParticipants.csv\nContains information about the residents of City of Engagement that have agreed to participate in this study.\n\nparticipantId (integer): unique ID assigned to each participant.\nhouseholdSize (integer): the number of people in the participant’s household\nhaveKids (boolean): whether there are children living in the participant’s household.\nage (integer): participant’s age in years at the start of the study.\neducationLevel (string factor): the participant’s education level, one of: {“Low”, “HighSchoolOrCollege”, “Bachelors”, “Graduate”}\ninterestGroup (char): a char representing the participant’s stated primary interest group, one of {“A”, “B”, “C”, “D”, “E”, “F”, “G”, “H”, “I”, “J”}. Note: specific topics of interest have been redacted to avoid bias.\njoviality (float): a value ranging from [0,1] indicating the participant’s overall happiness level at the start of the study.\n\nFinancialJournal.csv\nContains information about financial transactions.\n\nparticipantId (integer): unique ID corresponding to the participant affected\ntimestamp (datetime): the time when the check-in was logged\ncategory (string factor): a string describing the expense category, one of {“Education”, “Food”, “Recreation”, “RentAdjustment”, “Shelter”, “Wage”}\namount (double): the amount of the transaction\n\nFor explanation of Rent Adjustment, please refer to this link."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#load-packages-and-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#load-packages-and-data",
    "title": "Take-home_Ex01",
    "section": "2. Load packages and data:",
    "text": "2. Load packages and data:\nThe data should be processed by using appropriate tidyverse family of packages and the statistical graphics must be prepared using ggplot2 and its extensions.\n\n\nShow the code\npacman::p_load(ggstatsplot, tidyverse, ggplot2, plotly, ggmosaic, ggridges, gganimate,ggdist)\n\nfi <- read.csv(\"data/FinancialJournal.csv\")\nparti <- read.csv(\"data/Participants.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-checkingcleaning",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-checkingcleaning",
    "title": "Take-home_Ex01",
    "section": "3. Data checking/cleaning:",
    "text": "3. Data checking/cleaning:\n\n3.1 Check the summary of tables to get a whole picture of the data:\n\n\nShow the code\nsummary(fi)\n\n\n participantId     timestamp           category             amount         \n Min.   :   0.0   Length:1513636     Length:1513636     Min.   :-1562.726  \n 1st Qu.: 222.0   Class :character   Class :character   1st Qu.:   -5.594  \n Median : 464.0   Mode  :character   Mode  :character   Median :   -4.000  \n Mean   : 480.9                                         Mean   :   20.047  \n 3rd Qu.: 726.0                                         3rd Qu.:   21.598  \n Max.   :1010.0                                         Max.   : 4096.526  \n\n\nShow the code\nsummary(parti)\n\n\n participantId    householdSize    haveKids            age       \n Min.   :   0.0   Min.   :1.000   Mode :logical   Min.   :18.00  \n 1st Qu.: 252.5   1st Qu.:1.000   FALSE:710       1st Qu.:29.00  \n Median : 505.0   Median :2.000   TRUE :301       Median :39.00  \n Mean   : 505.0   Mean   :1.964                   Mean   :39.07  \n 3rd Qu.: 757.5   3rd Qu.:3.000                   3rd Qu.:50.00  \n Max.   :1010.0   Max.   :3.000                   Max.   :60.00  \n educationLevel     interestGroup        joviality       \n Length:1011        Length:1011        Min.   :0.000204  \n Class :character   Class :character   1st Qu.:0.240074  \n Mode  :character   Mode  :character   Median :0.477539  \n                                       Mean   :0.493794  \n                                       3rd Qu.:0.746819  \n                                       Max.   :0.999234  \n\n\n\n\n3.2 Some further check on each table\n\n\n3.2.1 Further check of fi table by summarizing by categories:\n\nIt’s confirmed from the summary table that “wages” and “rent adjustment” got only positive numbers and other categories are negative numbers.\n\n\n\nShow the code\nfi$month <- months.Date(strptime(fi$timestamp, \"%Y-%m-%d\"))\nfi$year <- format(strptime(fi$timestamp, \"%Y-%m-%d\"), format=\"%Y\")\nfi$yearmonth <- paste(fi$year,fi$month)\n\nfi %>%\n  group_by(year, category) %>%\n  summarise(median = median(amount),\n            mean = mean(amount),\n            min = min(amount),\n            max = max(amount),\n            n = n(),\n            neg = sum(amount<0))\n\n\n# A tibble: 11 × 8\n# Groups:   year [2]\n   year  category       median    mean       min      max      n    neg\n   <chr> <chr>           <dbl>   <dbl>     <dbl>    <dbl>  <int>  <int>\n 1 2022  Education       -38.0  -46.6    -91.1    -12.8     2825   2825\n 2 2022  Food             -4     -4.69   -14.8     -4     662378 662378\n 3 2022  Recreation      -14.4  -14.0    -44.5     -0.535 254501 254501\n 4 2022  RentAdjustment  246.   419.      77.2   1506.       131      0\n 5 2022  Shelter        -659.  -638.   -1563.    -232.      9703   9703\n 6 2022  Wage             83.9  112.       0.833 4097.    346531      0\n 7 2023  Education       -38.0  -46.2    -91.1    -12.8      494    494\n 8 2023  Food             -4     -4.67   -14.8     -4     127673 127673\n 9 2023  Recreation      -14.4  -14.0    -44.5     -0.535  41512  41512\n10 2023  Shelter        -654.  -635.   -1556.    -232.      1760   1760\n11 2023  Wage             83.9  105.       0.833  664.     66128      0\n\n\n\n\n\n\n\n\nWarning\n\n\n\nHere the Rent Adjustment amount which have much less N of records compare to other categories need to be carefully treated.\n\n\n\n\n3.2.2 Further check of parti table:\n\nDistribution of joviality:\n\n\nShow the code\nd <- highlight_key(parti)\np <-ggplot(data=parti, aes(x = joviality)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"white\", \n                 fill=\"#4575B4\") +\n  theme_light()+\n  ggtitle(\"Distribution of joviality\")\n\nhighlight(ggplotly(p),\"plotly_selected\")  \n\n\n\n\n\n\nDistribution of other variables:\n\n\nShow the code\nd <- highlight_key(parti)\np1 <- ggplot(data=d, aes(x = householdSize)) +\n  geom_bar(stat = \"count\", \n           boundary = 100,\n           color=\"white\", \n           fill=\"#4575B4\") +\n  xlab(\"Household Size\")\n\np2 <- ggplot(data=d, aes(x = age)) +\n  geom_bar(stat = \"count\", \n           boundary = 100,\n           color=\"white\", \n           fill=\"#ABD9E9\") +\n  xlab(\"Age\")\n\np3 <- ggplot(data=d, aes(x = educationLevel)) +\n  geom_bar(stat = \"count\", \n           boundary = 100,\n           color=\"white\", \n           fill=\"#FEE090\") +\n  xlab(\"Education Level\")+\n  theme(axis.text.x = element_text(angle = 30))\n\np4 <- ggplot(data=d, aes(x = interestGroup)) +\n  geom_bar(stat = \"count\", \n           boundary = 100,\n           color=\"white\", \n           fill=\"#F46D43\") +\n  xlab(\"Interest Group\")\n\nsubplot(ggplotly(p1),\n        ggplotly(p2),\n        ggplotly(p3),\n        ggplotly(p4),\n        nrows = 2,\n        titleX = TRUE,\n        margin = 0.1)\n\n\n\n\n\n\n\n\n\n3.3 Combine information into one table\n\nFind monthly average income and expenses of each participant ID;\nRe-categorize wage as “Income”, and all others together as “Expenses”;\nAnd for the convenience of latter using, I will create a column named “absave” to save the absolute number of the amount.\n\n\n\nShow the code\nfi_ave_by_cat <- fi%>%\n  group_by(participantId, category) %>%\n  summarise(total_amount = sum(amount),\n            n_months = length(unique(yearmonth)))\nfi_ave_by_cat$ave = ifelse(fi_ave_by_cat$category == 'RentAdjustment', \n                           fi_ave_by_cat$total_amount/12, \n                           fi_ave_by_cat$total_amount/fi_ave_by_cat$n_months)\n\nfi_ave_by_cat$bicat <- ifelse(fi_ave_by_cat$category == 'Wage','Income', 'Expenses')\nfi_ave_by_cat$absave <- abs(fi_ave_by_cat$ave)\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere I divided the amount of “Rent Adjustment” by 12 rather than the unique months of it’s record, that’s because this amount is an annual amount occurred only once in a year.\n\n\n\nThen summarize into a new table named “fi_ave” in which transpose the monthly average Income/Expenses categories into column names;\n\n\n\nShow the code\nfi_ave <- fi_ave_by_cat%>%\n  group_by(participantId, bicat) %>%\n  summarise(mon_ave = abs(sum(ave))) %>%\n  pivot_wider(names_from = bicat,values_from = mon_ave, names_prefix = \"mon_ave_\")\n\n\n\nCombine “fi_ave” with “parti” into a new table named “joy”.\n\n\n\nShow the code\njoy <- full_join(fi_ave,parti,by=\"participantId\")\nsummary(joy)\n\n\n participantId    mon_ave_Expenses mon_ave_Income  householdSize  \n Min.   :   0.0   Min.   :  32     Min.   : 1904   Min.   :1.000  \n 1st Qu.: 252.5   1st Qu.:1096     1st Qu.: 2563   1st Qu.:1.000  \n Median : 505.0   Median :1401     Median : 3416   Median :2.000  \n Mean   : 505.0   Mean   :1282     Mean   : 4061   Mean   :1.964  \n 3rd Qu.: 757.5   3rd Qu.:1650     3rd Qu.: 4805   3rd Qu.:3.000  \n Max.   :1010.0   Max.   :2713     Max.   :17645   Max.   :3.000  \n  haveKids            age        educationLevel     interestGroup     \n Mode :logical   Min.   :18.00   Length:1011        Length:1011       \n FALSE:710       1st Qu.:29.00   Class :character   Class :character  \n TRUE :301       Median :39.00   Mode  :character   Mode  :character  \n                 Mean   :39.07                                        \n                 3rd Qu.:50.00                                        \n                 Max.   :60.00                                        \n   joviality       \n Min.   :0.000204  \n 1st Qu.:0.240074  \n Median :0.477539  \n Mean   :0.493794  \n 3rd Qu.:0.746819  \n Max.   :0.999234"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#finding-demographic-characteristics-of-the-city",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#finding-demographic-characteristics-of-the-city",
    "title": "Take-home_Ex01",
    "section": "4.1 Finding demographic characteristics of the city",
    "text": "4.1 Finding demographic characteristics of the city\n\n4.1.1 How interest group varies by education levels?\nFrom the chart below we can find that different interest group attract people with quite different education level.\n\n\nShow the code\nd <- highlight_key(joy)\np <- ggplot(data = joy) +\n  geom_mosaic(aes(x = product(interestGroup), fill= educationLevel)) +\n  theme_mosaic() +\n  scale_fill_manual(values = c(\"#4575B4\", \"#ABD9E9\", \"#FEE090\", \"#F46D43\"))+\n  xlab(\"\")+\n  ylab(\"\")\n\nhighlight(ggplotly(p),\"plotly_selected\")  \n\n\n\n\n\n\n\n\n4.1.2 How interest group changes with people’s age?\n\n\nShow the code\nggplot(joy, aes(x = age, y = interestGroup, fill = interestGroup)) +\n  geom_density_ridges(alpha=0.8, color = \"white\") +\n  theme_ridges() + \n  theme(legend.position = \"none\",\n        panel.spacing = unit(0.1, \"lines\"),\n        strip.text.x = element_text(size = 8))+\n  scale_fill_manual(values = c(\"#F4EDCA\",\"#E7B800\", \"#C4961A\",  \"#D16103\", \"#C3D7A4\", \"#52854C\", \"#4E84C4\", \"#ABD9E9\", \"#FEE090\", \"#F46D43\")) +\n  xlab(\"Age\") +\n  ylab(\"Interest Groups\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#finding-financial-characteristics-of-the-city",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#finding-financial-characteristics-of-the-city",
    "title": "Take-home_Ex01",
    "section": "4.2 Finding financial characteristics of the city",
    "text": "4.2 Finding financial characteristics of the city\n\n4.2.1 How monthly average income varies between different education level groups\nCitizens’ monthly income apparently goes up with their educational level, indicating local labor market do care about employees’ education background.\n\n\nShow the code\nggbetweenstats(\n  data = joy,\n  x = educationLevel, \n  y = mon_ave_Income,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n4.2.2 Looks into monthly average income’s distribution together with other demographic information\nMonthly income distributing quite evenly over different household size and joviality level.\n\n\nShow the code\nd <- highlight_key(joy)\np <- ggplot(joy, aes(x = mon_ave_Income, y = joviality, \n                      size = householdSize, \n                      colour = haveKids)) +\n  geom_point(alpha = 0.6) +\n  scale_colour_manual(values = c(\"#4E84C4\", \"#FEE090\")) +\n  scale_size(range = c(1, 3))+\n  labs(title = 'Do people earn more feel happier?', \n       x = 'Monthly average income', \n       y = 'Joviality')\n\nhighlight(ggplotly(p),\"plotly_selected\")  \n\n\n\n\n\n\n\n\n4.2.3 How monthly expenses varies between different household size\nThe amount of monthly expenses of single household is quite different from other household size.\n\n\nShow the code\nggbetweenstats(\n  data = joy,\n  x = householdSize, \n  y = mon_ave_Expenses,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n4.2.4 Looks into monthly average expenses by categories\nAnd people’s most big amount of expense seems to be shelter expenses.\n\n\nShow the code\nfi_ave_by_cat %>%\n  filter(bicat != \"Income\" & category != \"RentAdjustment\") %>%\n  ggplot(aes(x = category, y = absave)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  theme_light() +\n  labs(\n    title = \"Confidence intervals of mean expense by category\",\n    subtitle = \"Mean Point + Multiple-interval plot\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#finding-correlation-between-characteristics-and-joviality",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#finding-correlation-between-characteristics-and-joviality",
    "title": "Take-home_Ex01",
    "section": "4.3 Finding correlation between characteristics and joviality",
    "text": "4.3 Finding correlation between characteristics and joviality\n\n4.3.1 Do people earn more feel happier?\nNo matter we look into the correlation between joviality and monthly income as a whole or by educational level group, the correlation is relatively week.\n\n\nShow the code\nggscatterstats(data = joy,\n                     x = mon_ave_Income,\n                     y = joviality,\n                     marginal = FALSE,\n                     point.args = list(size = 1, alpha = 0.6, stroke = 0),\n               title = \"Joviality negatively correlated with monthly average income\")\n\n\n\n\n\n\n\nShow the code\ngrouped_ggscatterstats(\n  data = joy,\n  x = mon_ave_Income,\n  y = joviality,\n  marginal = FALSE,\n  point.args = list(size = 1, alpha = 0.6, stroke = 0),\n  grouping.var = educationLevel,\n  type = \"r\",\n  annotation.args = list(title = \"Relationship between joviality and income by education level\"),\n  plotgrid.args = list(nrow = 2, ncol = 2)\n)\n\n\n\n\n\n\n\n4.3.2 Or do people spend more feel happier?\n\nTo look at the correlation between joviality and monthly average expenses, seemingly there is only a weakly positive correlation.\n\n\n\nShow the code\nggscatterstats(data = joy,\n                    x = mon_ave_Expenses,\n                    y = joviality,\n                    marginal = FALSE,\n                    point.args = list(size = 1, alpha = 0.6, stroke = 0),\n               title = \"Joviality positively correlated with monthly average expenses\")\n\n\n\n\n\n\nFunny part is, when we look at this correlation by different household size, the correlation between joviality and monthly average expenses is much stronger for size 1 households, indicating single persons enjoy spending money!\n\n\n\nShow the code\ngrouped_ggscatterstats(\n  ## arguments relevant for ggscatterstats\n  data = joy,\n  x = mon_ave_Expenses,\n  y = joviality,\n  marginal = FALSE,\n  point.args = list(size = 1, alpha = 0.6, stroke = 0),\n  grouping.var = householdSize,\n  type = \"r\", \n  annotation.args = list(title = \"Relationship between joviality and expenses by household size\"),\n  plotgrid.args = list(nrow = 1)\n)\n\n\n\n\n\n\nThen let’s dig deeper to see how single person’s spend their money.\nUnlike what we have seen in analysis of whole data set which indicating people spend the most on shelter, the average amount single citizens spend on food and recreation comes much closer to the average amount spend on shelter.\n\n\nShow the code\nsingleId <- joy %>%\n  filter(householdSize == 1) %>%\n  subset(select = \"participantId\")\n\nexpense_cat <- fi_ave_by_cat%>%\n  filter(bicat == \"Expenses\" & category != \"RentAdjustment\") %>%\n  subset(select = c(\"participantId\", \"category\", \"absave\")) %>%\n  group_by(participantId, category) %>%\n  summarise(absave = sum(absave))\n\nsingle <- left_join(singleId, expense_cat, by = \"participantId\")\n\nsingle %>%\n  ggplot(aes(x = category, y = absave)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  theme_light() +\n  labs(\n    title = \"Confidence intervals of single citizens' mean expense by category\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\nIf this is the case, do person’s joviality correlated with their spending on recreation?\nYes, they do!\n\n\nShow the code\njoy_withcat <- left_join(joy, \n                 expense_cat %>% pivot_wider(names_from = category, names_prefix = \"mon_ave_\", values_from = absave),\n                 by = \"participantId\")\n\nggscatterstats(data = joy_withcat,\n                    x = mon_ave_Recreation,\n                    y = joviality,\n                    marginal = FALSE,\n                    point.args = list(size = 1, alpha = 0.6, stroke = 0),\n               title = \"Joviality positively correlated with monthly average spending on recreation\")\n\n\n\n\n\nHow about expenses on food?\nNo, they don’t.\n\n\nShow the code\nggscatterstats(data = joy_withcat,\n                    x = mon_ave_Food,\n                    y = joviality,\n                    marginal = FALSE,\n                    point.args = list(size = 1, alpha = 0.6, stroke = 0),\n               title = \"Joviality positively correlated with monthly average spending on recreation\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html",
    "title": "Take-home_Ex03",
    "section": "",
    "text": "FishEye International, a non-profit focused on countering illegal, unreported, and unregulated (IUU) fishing, has been given access to an international finance corporation’s database on fishing related companies. In the past, FishEye has determined that companies with anomalous structures are far more likely to be involved in IUU (or other “fishy” business). FishEye has transformed the database into a knowledge graph. It includes information about companies, owners, workers, and financial status. FishEye is aiming to use this graph to identify anomalies that could indicate a company is involved in IUU.\nFishEye analysts have attempted to use traditional node-link visualizations and standard graph analyses, but these were found to be ineffective because the scale and detail in the data can obscure a business’s true structure. Can you help FishEye develop a new visual analytics approach to better understand fishing business anomalies?\n\n\n\nUse visual analytics to understand patterns of groups in the knowledge graph and highlight anomalous groups.\n\nUse visual analytics to identify anomalies in the business groups present in the knowledge graph. Limit your response to 400 words and 5 images.\nDevelop a visual analytics process to find similar businesses and group them. This analysis should focus on a business’s most important features and present those features clearly to the user. Limit your response to 400 words and 5 images.\nMeasure similarity of businesses that you group in the previous question. Express confidence in your groupings visually. Limit your response to 400 words and 4 images.\nBased on your visualizations, provide evidence for or against the case that anomalous companies are involved in illegal fishing. Which business groups should FishEye investigate further? Limit your response to 600 words and 6 images.\n\n\n\n\n\n\n\nPlease be noticed:\n\n\n\nIn this exercise, only question 1 and question 2 would be explored."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#find-the-nodes-and-edges",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#find-the-nodes-and-edges",
    "title": "Take-home_Ex03",
    "section": "2.1 Find the nodes and edges:",
    "text": "2.1 Find the nodes and edges:\n\n2.1.1 Nodes\n\n\nShow the code\n#view(mc2[[\"nodes\"]])\nmc3_nodes <- as_tibble(mc3$nodes) %>%\n  mutate(country=as.character(country),\n         id=as.character(id),\n         product_services=as.character(product_services),\n         revenue_omu = as.numeric(as.character(revenue_omu)),\n         type=as.character(type)) %>%\n  select(id,country, type, revenue_omu, product_services) \n#  group_by(id,country, type, product_services) %>%\n#  summarise(count=n(),revenue=sum(revenue_omu))\n\n\n\n\nShow the code\nskim(mc3_nodes)\n\n\n\nData summary\n\n\nName\nmc3_nodes\n\n\nNumber of rows\n27622\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nid\n0\n1\n6\n64\n0\n22929\n0\n\n\ncountry\n0\n1\n2\n15\n0\n100\n0\n\n\ntype\n0\n1\n7\n16\n0\n3\n0\n\n\nproduct_services\n0\n1\n4\n1737\n0\n3244\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrevenue_omu\n21515\n0.22\n1822155\n18184433\n3652.23\n7676.36\n16210.68\n48327.66\n310612303\n▇▁▁▁▁\n\n\n\n\n\n\n\n2.1.2 Edges:\n\n\nShow the code\n#view(mc2[[\"links\"]])\nmc3_edges <- as_tibble(mc3$links) %>%\n  distinct() %>%\n  mutate(source=as.character(source),\n         target=as.character(target),\n         type=as.character(type)) %>%\n  mutate(source=as.character(source)) %>%\n  group_by(source, target, type) %>%\n  summarise(weight=n()) %>%\n  filter(source!=target) %>%\n  ungroup()\n\n\n\n\nShow the code\nmc3_edges <- rbind(mc3_edges %>%\n  mutate(source = ifelse(grepl(\"^c\\\\(.*\\\\)$\", source), source, NA)) %>%\n  drop_na() %>%\n  mutate(source = strsplit(gsub(\"^c\\\\(\\\"|\\\"\\\\)$|\\\"\", \"\", source), \",\\\\s*\")) %>%\n  unnest(source),\n  mc3_edges[!grepl(\"^c\\\\(.*\\\\)$\", mc3_edges$source), ])\n\n\n\n\nShow the code\nskim(mc3_edges)\n\n\n\nData summary\n\n\nName\nmc3_edges\n\n\nNumber of rows\n27169\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsource\n0\n1\n6\n64\n0\n13158\n0\n\n\ntarget\n0\n1\n6\n28\n0\n21265\n0\n\n\ntype\n0\n1\n16\n16\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nweight\n0\n1\n1\n0\n1\n1\n1\n1\n1\n▁▁▇▁▁\n\n\n\n\n\n\n\n\n\n\n\nCaution Here:\n\n\n\nThere are plenty of rows in “source” column are comma separated characters, I broke them into individual rows by deliminator of “,”, remove all those “c(”“)”, and then rebind with other normal columns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-the-edge-data-frame.",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-the-edge-data-frame.",
    "title": "Take-home_Ex03",
    "section": "3.1 Exploring the edge data frame.",
    "text": "3.1 Exploring the edge data frame.\n\nCheck the type distribution of edges:\n\n\n\nShow the code\nmc3_edges %>%\n  group_by(type) %>%\n  summarise(count=n())\n\n\n# A tibble: 2 × 2\n  type             count\n  <chr>            <int>\n1 Beneficial Owner 18691\n2 Company Contacts  8478"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-the-nodes-data-frame.",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#exploring-the-nodes-data-frame.",
    "title": "Take-home_Ex03",
    "section": "3.2 Exploring the nodes data frame.",
    "text": "3.2 Exploring the nodes data frame.\n\nCheck products & services of each kind of nodes:\n\n\n\nShow the code\nproduct_check <- left_join(mc3_nodes %>%\n  group_by(type) %>%\n  summarise(nodes=n()),\n  mc3_nodes %>%\n  mutate(n_0 = str_count(product_services, \"character\")) %>%\n  filter(n_0>0) %>%\n  group_by(type) %>%\n  summarise(empty_product=n()),\n  by=join_by(type),\n  keep=FALSE)%>%\n  mutate(nodes_with_product=nodes-empty_product)\n\nproduct_check\n\n\n# A tibble: 3 × 4\n  type             nodes empty_product nodes_with_product\n  <chr>            <int>         <int>              <int>\n1 Beneficial Owner 11949         11926                 23\n2 Company           8639             1               8638\n3 Company Contacts  7034          7033                  1\n\n\n\n\n\n\n\n\nCaution Here:\n\n\n\nMost of Beneficial Owner and Company Contacts nodes’ product_services are indicated as “character(0)”, so only “Company” type of nodes got specific product and services description."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#text-sensing-and-nodes-categorization",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#text-sensing-and-nodes-categorization",
    "title": "Take-home_Ex03",
    "section": "3.3 Text sensing and nodes categorization:",
    "text": "3.3 Text sensing and nodes categorization:\n\n3.3.1 Extract text and preprocessing.\n\nExtract text\n\n\n\nShow the code\nmc3_nodes %>%\n  select(product_services) %>%\n  group_by(product_services) %>%\n  summarise(count = n())\n\n\n# A tibble: 3,244 × 2\n   product_services                                                        count\n   <chr>                                                                   <int>\n 1 (Italian) peeled tomatoes, legumes, vegetables, fruits and canned mush…     1\n 2 100 percent Spanish olives; peppers, green, black, and manzanilla stuf…     1\n 3 2 or 3-piece containers, twist off caps, easy opening and traditional …     1\n 4 8 Cement Mixer Units, Ocean Freight, Air Freight, Project Logistics, C…     1\n 5 A chemical science firm with a focus on the development of high purity…     1\n 6 A complete range of fully-vertical, Schiffli embroidery manufacturing …     1\n 7 A complete range of transportation and logistics services                   1\n 8 A customs broker and freight forwarder                                      1\n 9 A distributor, importer and exporter of food products to the food reta…     1\n10 A freight broker                                                            1\n# ℹ 3,234 more rows\n\n\n\nWord tokenization with punctuation and no lowercasing\n\n\n\nShow the code\ntidy_nodes <- mc3_nodes %>%\n  unnest_tokens(word, product_services, to_lower = TRUE, strip_punct = TRUE)\n\n\n\nRemoving stop words\n\n\n\nShow the code\ntidy_stopwords <- tidy_nodes %>%\n  anti_join(stop_words) #%>%\n#  anti_join(by = c(\"character\",\"0\",\"unknown\"))\n\n\n\n\n\n\n\n\nNote:\n\n\n\nHere need to also remove “character”,“0”,“unknown” as stop words.\n\n\n\n\n3.3.2 Find nodes’ product services mainly focus on “fish” or “transportation”:\n\nnodes’ product services mainly focus on “fish”\n\n\n\nShow the code\nmc3_nodes %>%\n  mutate(n_fish = str_count(product_services), \"fish\")\n\n\n# A tibble: 27,622 × 7\n   id                 country type  revenue_omu product_services n_fish `\"fish\"`\n   <chr>              <chr>   <chr>       <dbl> <chr>             <int> <chr>   \n 1 Jones LLC          ZH      Comp…  310612303. Automobiles          11 fish    \n 2 Coleman, Hall and… ZH      Comp…  162734684. Passenger cars,…     39 fish    \n 3 Aqua Advancements… Oceanus Comp…  115004667. Holding firm wh…    248 fish    \n 4 Makumba Ltd. Liab… Utopor… Comp…   90986413. Car service, ca…    428 fish    \n 5 Taylor, Taylor an… ZH      Comp…   81466667. Fully electric …     72 fish    \n 6 Harmon, Edwards a… ZH      Comp…   75070435. Discount superm…     59 fish    \n 7 Punjab s Marine c… Riodel… Comp…   72167572. Beef, pork, chi…    652 fish    \n 8 Assam   Limited L… Utopor… Comp…   72162317. Power and Gas s…   1737 fish    \n 9 Ianira Starfish S… Rio Is… Comp…   68832979. Light commercia…     94 fish    \n10 Moran, Lewis and … ZH      Comp…   65592906. Automobiles, tr…     88 fish    \n# ℹ 27,612 more rows\n\n\n\nnodes’ product services mainly focus on “transportation”, “warehouse”, “trad”,“commercial”:\n\n\n\nShow the code\nmc3_nodes %>%\n  mutate(n_logistic = str_count(product_services), \"transportation\")\n\n\n# A tibble: 27,622 × 7\n   id                      country type  revenue_omu product_services n_logistic\n   <chr>                   <chr>   <chr>       <dbl> <chr>                 <int>\n 1 Jones LLC               ZH      Comp…  310612303. Automobiles              11\n 2 Coleman, Hall and Lopez ZH      Comp…  162734684. Passenger cars,…         39\n 3 Aqua Advancements Sash… Oceanus Comp…  115004667. Holding firm wh…        248\n 4 Makumba Ltd. Liability… Utopor… Comp…   90986413. Car service, ca…        428\n 5 Taylor, Taylor and Far… ZH      Comp…   81466667. Fully electric …         72\n 6 Harmon, Edwards and Ba… ZH      Comp…   75070435. Discount superm…         59\n 7 Punjab s Marine conser… Riodel… Comp…   72167572. Beef, pork, chi…        652\n 8 Assam   Limited Liabil… Utopor… Comp…   72162317. Power and Gas s…       1737\n 9 Ianira Starfish Sagl I… Rio Is… Comp…   68832979. Light commercia…         94\n10 Moran, Lewis and Jimen… ZH      Comp…   65592906. Automobiles, tr…         88\n# ℹ 27,612 more rows\n# ℹ 1 more variable: `\"transportation\"` <chr>\n\n\n\ncheck nodes’ category by their product services"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#visualizing-temporal-patterns-for-individual-entities-by-heatmap",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#visualizing-temporal-patterns-for-individual-entities-by-heatmap",
    "title": "Take-home_Ex03",
    "section": "4.1 Visualizing temporal patterns for individual entities by heatmap",
    "text": "4.1 Visualizing temporal patterns for individual entities by heatmap\n\n4.1.1 Transforming the data frame into a matrix\nFind edges filtering by those majority hscodes.\n\n\n\n\n\n\n\n\n4.1.2 Building heatmap"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#visualizing-links-between-companies",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#visualizing-links-between-companies",
    "title": "Take-home_Ex03",
    "section": "4.2 Visualizing links between companies",
    "text": "4.2 Visualizing links between companies\n\n4.2.1 Build a network graph of those selected hscodes with most fishing weight:\n\nPrepare the data and build a graph.\n\n\n\n\n\n\n\n\nBasic network graph to see the whole picture.\n\n\n\n\n\nBuild a facet graph by years to see the change of network through years:\n\n\n\n\n\nBuild a facet network graph by hscode:\n\n\n\n\n\nBuild a facet_nodes graph by hscode to see the difference:\n\n\n\n\n\n\n4.2.2 Find communities\n\n\n\n\n\n4.2.3 Build interactive network graph.\nPrepare data for interactive network graph.\n\n\n\nBuild an interactive network graph for checking the position of each node."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#evaluate-the-sets-of-predicted-knowledge-graph-links-fisheye-has-provided",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#evaluate-the-sets-of-predicted-knowledge-graph-links-fisheye-has-provided",
    "title": "Take-home_Ex03",
    "section": "4.3 Evaluate the sets of predicted knowledge graph links FishEye has provided",
    "text": "4.3 Evaluate the sets of predicted knowledge graph links FishEye has provided\n\n4.3.1 Read provided json files:\nFirstly read all 12 files provided by Fisheye into one table.\n\n\n\n\n\n\n\n\n\nThen check number of edges by “hscode” and by “generagted_by” (here I renamed this column as “group”)\n\n\n\n\n\n4.3.2 Data wrangling for network graph:\n\n\n\n\n\n\n\n\n4.3.3 Visualize graph provided by Fisheye:\n\n\n\n\nFacet node graph by groups:\n\n\n\n\n\nFacet network graph by groups:\n\n\n\n\n\n\n\n\n\n\nConclusion:\n\n\n\nHere I will select “carp” group as the new set of links to add into mc2 graph, since from the facet nodes graph I can see this set of link got most and sparsest of nodes, indicating this set should be able to contribute the most to the original graph. At the same time, the facet network graph show not much difference between groups."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "The country of Oceanus has sought FishEye International’s help in identifying companies possibly engaged in illegal, unreported, and unregulated (IUU) fishing. As part of the collaboration, FishEye’s analysts received import/export data for Oceanus’ marine and fishing industries. However, Oceanus has informed FishEye that the data is incomplete. To facilitate their analysis, FishEye transformed the trade data into a knowledge graph. Using this knowledge graph, they hope to understand business relationships, including finding links that will help them stop IUU fishing and protect marine species that are affected by it. FishEye analysts found that node-link diagrams gave them a good high-level overview of the knowledge graph. However, they are now looking for visualizations that provide more detail about patterns for entities in the knowledge graph. There are two main parts to this analysis.\nFirst, FishEye knows from past experience that companies caught fishing illegally will shut down but will then often start up again under a different name. FishEye wants your help to visualize temporal patterns so they can compare the activities of companies over time to determine if the companies have returned to their nefarious acts.\nSecond, FishEye has been using several tools, including artificial intelligence, to reason on the knowledge graph and suggest links that could extend the dataset. They have supplied 12 groups of link suggestions and need your help evaluating these groups to identify which tools are most reliable for completing the graph. FishEye is especially interested in identifying new temporal patterns or anomalies that are only present when new links are added.\n\n\n\n\nUse visual analytics to identify temporal patterns for individual entities and between entities in the knowledge graph FishEye created from trade records. Categorize the types of business relationship patterns you find. Limit your response to 600 words and 6 images.\nEvaluate the sets of predicted knowledge graph links FishEye has provided using visual analytics. Which sets are most reliable for completing the graph? Limit your response to 600 words and 6 images.\nIllustrate how your visual analytics approach can be used to identify new patterns and/or anomalies that are present in the knowledge graph after you have added the links you deemed reliable in question 2. Limit your response to 300 words and 4 images.\nIdentify companies that fit a pattern of illegal fishing. Use visualizations to support your conclusions and your confidence in them. Limit your response to 300 words and 4 images.\n\n\n\n\n\n\n\nNote:\n\n\n\nOnly Question 1 would be explored in this Exercise."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#find-the-nodes-and-edges",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#find-the-nodes-and-edges",
    "title": "Take-home_Ex02",
    "section": "3.1 Find the nodes and edges:",
    "text": "3.1 Find the nodes and edges:\n\n\nShow the code\n#view(mc2[[\"nodes\"]])\nmc2_nodes <- as_tibble(mc2$nodes) %>%\n  select(id, shpcountry, rcvcountry) %>%\n  distinct()\n\n\n\n\nShow the code\n#view(mc2[[\"links\"]])\nmc2_edges <- as_tibble(mc2$links) %>%\n  mutate(arrivaldate = ymd(arrivaldate)) %>%\n  mutate(year = year(arrivaldate)) %>%\n  select(source, target, weightkg, hscode, arrivaldate, year) %>%\n  drop_na() %>%\n  distinct()\n\nmc2_edges <-  mc2_edges %>%\n  mutate(from = with(mc2_nodes, id[match(mc2_edges$source, id)]))%>%\n  mutate(to = with(mc2_nodes, id[match(mc2_edges$target, id)])) %>%\n  distinct()\n\n\n\n\nShow the code\nglimpse(mc2_edges)\n\n\nRows: 5,190,407\nColumns: 8\n$ source      <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son's\", …\n$ target      <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica Marine…\n$ weightkg    <int> 4780, 6125, 10855, 11250, 11165, 11290, 9000, 19490, 6865,…\n$ hscode      <chr> \"630630\", \"630630\", \"470710\", \"470710\", \"470710\", \"470710\"…\n$ arrivaldate <date> 2034-02-12, 2034-03-13, 2028-02-07, 2028-02-23, 2028-09-1…\n$ year        <dbl> 2034, 2034, 2028, 2028, 2028, 2028, 2028, 2028, 2028, 2028…\n$ from        <chr> \"AquaDelight Inc and Son's\", \"AquaDelight Inc and Son's\", …\n$ to          <chr> \"BaringoAmerica Marine Ges.m.b.H.\", \"BaringoAmerica Marine…\n\n\n\n\n\n\n\n\nNote:\n\n\n\nHere the ‘arrivaldate’ column of edges is treated as ‘chr’ datatype, would need to be changed to ‘date’ type. And a new column added to indicate years."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#checking-data-distributions",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#checking-data-distributions",
    "title": "Take-home_Ex02",
    "section": "3.2 Checking data distributions:",
    "text": "3.2 Checking data distributions:\n\nDistribution of shipments and weight(kg) by year:\n\n\n\nShow the code\nshipping <- mc2_edges %>%\n         select(year, weightkg) %>%\n         group_by(year) %>%\n         summarise(count=n(),weightkg = sum(weightkg),kg_per_ship=weightkg/count)\n\nd <- highlight_key(shipping)\np1 <- ggplot(data=d, \n            aes(x = year,\n                y = count)) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Year\") +\n  ylab(\"Count\")\n\np2 <- ggplot(data=d, \n            aes(x = year,\n                y = weightkg)) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Year\") +\n  ylab(\"Weightkg\")\n\np3 <- ggplot(data=d, \n            aes(x = year,\n                y = kg_per_ship)) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Year\") +\n  ylab(\"average kg\")\n\nsubplot(ggplotly(p1),\n        ggplotly(p2),\n        ggplotly(p3),\n        shareX = TRUE,\n        nrows = 3,\n        titleY = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\n\nHere we can conclude from the distribution of shipment by year that the weight per shipment of year 2032 is significantly higher than other years, indicating possible abnormal fishing volume in this year. For later using, I will focus on those hscode with most number of shipment during 2032.\n\n\n\nCheck number of shipments and fishing weights in 2032 by hscode.\n\n\n\nShow the code\nhscode_count <-  mc2_edges %>%\n  filter(year==2032) %>%\n  select(hscode,weightkg) %>%\n  group_by(hscode) %>%\n  summarise(weightkg = sum(weightkg), count=n(), kg_per_ship=weightkg/count) %>%\n  distinct() %>%\n  arrange(desc(count))\nhscode_count\n\n\n# A tibble: 3,864 × 4\n   hscode   weightkg count kg_per_ship\n   <chr>       <dbl> <int>       <dbl>\n 1 306170  477204630 24772      19264.\n 2 940360  181475405 14794      12267.\n 3 870899  479390615 14255      33630.\n 4 611020   75241440 13321       5648.\n 5 950300  105203525 12274       8571.\n 6 304620  216078950  9958      21699.\n 7 870323 2578623605  9313     276884.\n 8 640299   86822135  8815       9849.\n 9 160414  477284410  8762      54472.\n10 940161   92760155  8528      10877.\n# ℹ 3,854 more rows\n\n\nShow the code\nhscode_topcount <- pull(head(hscode_count, 1),hscode)\nhscode_top_avekg <- pull(head(hscode_count%>%\n                                filter(count>1000) %>%\n                                arrange(desc(kg_per_ship)),1),hscode)\n\nsprintf(\"hscode with highest number of shipment: %s\", hscode_topcount)\n\n\n[1] \"hscode with highest number of shipment: 306170\"\n\n\nShow the code\nsprintf(\"hscode with highest weight(kg) per shipment: %s\", hscode_top_avekg)\n\n\n[1] \"hscode with highest weight(kg) per shipment: 721049\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualizing-temporal-patterns-for-individual-entities-by-heatmap",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualizing-temporal-patterns-for-individual-entities-by-heatmap",
    "title": "Take-home_Ex02",
    "section": "4.1 Visualizing temporal patterns for individual entities by heatmap",
    "text": "4.1 Visualizing temporal patterns for individual entities by heatmap\n\n4.1.1 Preparing data\nFilter out those id with more than 20K count of shipements and transform the data table into matrix.\n\n\nShow the code\nmc2_selected_id <- pull(mc2_edges %>% \n                                 select(source) %>% \n                                 group_by(source) %>%\n                                 summarize(count=n()) %>%\n                                 filter(count > 20000) %>%\n                                 distinct() %>%\n                                 rename(id=source), \n                               id)\n\nmc2_selected_companies <- mc2_edges %>%\n  filter(source %in% mc2_selected_id) %>%\n  select(source,year) %>%\n  group_by(source,year) %>%\n  summarize(shipment=n()) %>%\n  pivot_wider(names_from = year, values_from = shipment, values_fill = 0) \n\nrow.names(mc2_selected_companies) <- mc2_selected_companies$source\ncompanies_matrix <- data.matrix(mc2_selected_companies)\n\n\n\n\n4.1.2 Building heatmap\n\n\nShow the code\nheatmaply(normalize(companies_matrix[, -c(1, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 6,\n          fontsize_col = 6,\n          main=\"Companies' yearly trend of shipment counts\",\n          xlab = \"Year\",\n          ylab = \"Companies\"\n          )\n\n\n\n\n\n\n\n\n\n\n\n\nPlease mind here:\n\n\n\nThere are several companies have abrupt change in number of shipment through years. This could indicating abnormal like change of identity or abnormal fishing activities."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualizing-networks",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#visualizing-networks",
    "title": "Take-home_Ex02",
    "section": "4.2 Visualizing networks",
    "text": "4.2 Visualizing networks\n\n4.2.1 Preparing data:\n\nFiltering edges by 2032 and set count as weights.\n\n\n\nShow the code\nmc2_edges_aggregated <- mc2_edges %>%\n  filter(year == 2032) %>%\n  group_by(source, target, hscode, from, to) %>%\n  summarise(weights = n(), ave_kg = sum(weightkg)/weights) %>%\n  filter(source!=target) %>%\n  ungroup()\n\nglimpse(mc2_edges_aggregated)\n\n\nRows: 152,980\nColumns: 7\n$ source  <chr> \" Direct Herring Company Transit\", \" Direct Herring Company Tr…\n$ target  <chr> \"Caracola Azul NV Nautical\", \"Caracola Azul NV Nautical\", \"Lim…\n$ hscode  <chr> \"160529\", \"306170\", \"841430\", \"330430\", \"392310\", \"392330\", \"3…\n$ from    <chr> \" Direct Herring Company Transit\", \" Direct Herring Company Tr…\n$ to      <chr> \"Caracola Azul NV Nautical\", \"Caracola Azul NV Nautical\", \"Lim…\n$ weights <int> 3, 2, 4, 1, 1, 1, 3, 1, 1, 1, 1, 2, 2, 9, 9, 1, 1, 1, 1, 1, 2,…\n$ ave_kg  <dbl> 10745.000, 18422.500, 3725.000, 320.000, 6645.000, 6645.000, 1…\n\n\n\n\n4.2.2 Build network graph for hscode with highest shipment counts:\n\nFind edges of hscode equal to “306170”. And focus on shipments more than once a month (i.e. count of shipments >= 12).\n\n\n\nShow the code\nmc2_edges_topcount <- mc2_edges_aggregated %>%\n  filter(hscode == hscode_topcount) %>%\n  group_by(from, to) %>%\n  summarise(weights = sum(weights)) %>%\n  filter(from!=to) %>%\n  filter(weights>12) %>%\n  ungroup\n\n\n\nExtract nodes. Check source and target companies, categorize them into “fisher” (nodes in “from” column) or “wholesaler” (nodes in “to” column).\n\n\n\nShow the code\nmc2_nodes_topcount <- rbind(mc2_edges_topcount %>%\n                                select(from)%>%\n                                rename(id=from) %>%\n                             mutate(group=\"fisher\"),\n                              mc2_edges_topcount %>%\n                                select(to)%>%\n                                rename(id=to) %>%\n                             mutate(group=\"wholesaler\")) %>% distinct()\n\n\n\nCheck any nodes been categorized into both “fisher” and “wholesaler”, mutate their group by higher weights.\n\n\n\nShow the code\nlookup_topcount <- mc2_nodes_topcount %>%\n  group_by(id) %>%\n  summarise(count=n()) %>%\n  filter(count>1) %>%\n  left_join(mc2_edges_topcount%>%\n              group_by(from)%>%\n              summarise(from_weights= sum(weights)),\n            by=c(\"id\"=\"from\")) %>%\n  left_join(mc2_edges_topcount%>%\n              group_by(to)%>%\n              summarise(to_weights=sum(weights)),\n            by=c(\"id\"=\"to\")) %>%\n  mutate(group=ifelse(from_weights >= to_weights, \"fisher\",\"wholesaler\"))\n\nmc2_nodes_topcount <- mc2_nodes_topcount %>%\n  left_join(lookup_topcount%>%select(id,group),by=\"id\") %>%\n  mutate(group = ifelse(!is.na(group.y),group.y,group.x)) %>%\n  select(-group.y,-group.x) %>%\n  distinct()\n\n\n\nCheck mutual-exclusiveness of category.\n\n\n\nShow the code\nmc2_nodes_topcount %>%\n  group_by(id) %>%\n  summarise(count=n())%>%\n  filter(count>1)\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: id <chr>, count <int>\n\n\n\nBuild network graph.\n\n\n\nShow the code\nmc2_graph_topcount <- tbl_graph(nodes = mc2_nodes_topcount,\n                       edges = mc2_edges_topcount,\n                       directed = TRUE)\n\n\n\n\nShow the code\nmc2_graph_topcount %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(width=weights), \n                 alpha=0.8, color=\"lightblue\") +\n  scale_edge_width(range = c(0.5, 3)) +\n  geom_node_point(aes(color=group, \n                  size = centrality_degree(mode = \"out\"))) + \n  scale_colour_manual(values=c(\"salmon\",\"steelblue\")) +\n  labs(size= \"Out_degree\")+\n  geom_node_text(aes(label=id), show.legend = FALSE, size=1) +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nNotice\n\n\n\nHere I used out-degree centrality as nodes’ size to focus more on shippers. We can tell from the graph, even if filtered by weights more than 12, it’s still a very condensed network, indicating this hscode mainly contains a very popular product which may attract illegal fish activities and calls for more attention.\n\n\n\nCheck nodes with out-degree >= 10, those fisher transit products to many different nodes, it’s possible that they conducted unreported fishing and tried to hide it in splitted transactions.\n\n\n\nShow the code\nmc2_graph_topcount %>%\n    mutate(centrality = centrality_degree(mode = \"out\")) %>%\n    filter(centrality>=10) %>%\n    select(id,group) \n\n\n# A tbl_graph: 7 nodes and 0 edges\n#\n# A rooted forest with 7 trees\n#\n# A tibble: 7 × 2\n  id                          group \n  <chr>                       <chr> \n1 Balkan Cat ОАО Transport    fisher\n2 Blue Horizon Family &       fisher\n3 Daniel Ferry N.V. Transit   fisher\n4 Lake Tana  & Son's          fisher\n5 Playa de Arena OJSC Express fisher\n6 The Salty Dog LLC           fisher\n# ℹ 1 more row\n#\n# A tibble: 0 × 3\n# ℹ 3 variables: from <int>, to <int>, weights <int>\n\n\n\nbuild an interactive network graph for closer check of nodes.\n\n\n\nShow the code\nvisNetwork(mc2_nodes_topcount,mc2_edges_topcount) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\nThen let’s compare with the network graph of those edges with only 1 shipment during the year:\n\n\n\nShow the code\nmc2_edges_topcount_1time <- mc2_edges_aggregated %>%\n  filter(hscode == hscode_topcount) %>%\n  group_by(from, to) %>%\n  summarise(weights = sum(weights)) %>%\n  filter(from!=to) %>%\n  filter(weights==1) %>%\n  ungroup\n\n\n\n\nShow the code\n#categarize nodes into 2 groups and make sure the group label is mutual exclusive\nmc2_nodes_topcount_1time <- rbind(mc2_edges_topcount_1time %>%\n                                select(from)%>%\n                                rename(id=from) %>%\n                             mutate(group=\"fisher\"),\n                              mc2_edges_topcount_1time %>%\n                                select(to)%>%\n                                rename(id=to) %>%\n                             mutate(group=\"wholesaler\")) %>% distinct()\n\nlookup_topcount_1time <- mc2_nodes_topcount_1time %>%\n  group_by(id) %>%\n  summarise(count=n()) %>%\n  filter(count>1) %>%\n  left_join(mc2_edges_topcount_1time%>%\n              group_by(from)%>%\n              summarise(from_weights= sum(weights)),\n            by=c(\"id\"=\"from\")) %>%\n  left_join(mc2_edges_topcount_1time%>%\n              group_by(to)%>%\n              summarise(to_weights=sum(weights)),\n            by=c(\"id\"=\"to\")) %>%\n  mutate(group=ifelse(from_weights >= to_weights, \"fisher\",\"wholesaler\"))\n\nmc2_nodes_topcount_1time <- mc2_nodes_topcount_1time %>%\n  left_join(lookup_topcount_1time%>%select(id,group),by=\"id\") %>%\n  mutate(group = ifelse(!is.na(group.y),group.y,group.x)) %>%\n  select(-group.y,-group.x) %>%\n  distinct()\n\n\n\n\nShow the code\n#build interactive network graph\nvisNetwork(mc2_nodes_topcount_1time,mc2_edges_topcount_1time) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice\n\n\n\nFrom this graph of those edges only once in a year, we can see the number of fisers (blue points) is far more than frequent (>=12 times in a year) fishers in upper graph. Since we have concluded this hscode mainly contains a popular product, transit for only once in a year seems not a normal conduction.\n\n\n\n\n\n\n\n\nPlease mind here:\n\n\n\nHere from the graph of edges both with weights >=12 and weights == 1, we can see a basic pattern of transitions is that they are directed from multiple fishers to single wholesaler.\n\n\n\n\n4.2.3 Build network graph for hscodes with highest weight(kg) per shipment:\n\nFind edges of the hscodes.\n\n\n\nShow the code\nmc2_edges_topkg <- mc2_edges_aggregated %>%\n  filter(hscode == hscode_top_avekg) %>%\n  group_by(from, to) %>%\n  summarise(weights = sum(weights)) %>%\n  filter(from!=to) %>%\n#  filter(weights>1) %>%\n  ungroup\n\n\n\nExtract nodes. Check source and target companies, categorize them into “fisher” (nodes in “from” column) or “wholesaler” (nodes in “to” column).\n\n\n\nShow the code\nmc2_nodes_topkg <- rbind(mc2_edges_topkg %>%\n                                select(from)%>%\n                                rename(id=from) %>%\n                             mutate(group=\"fisher\"),\n                              mc2_edges_topkg %>%\n                                select(to)%>%\n                                rename(id=to) %>%\n                             mutate(group=\"wholesaler\")) %>% distinct()\n\nlookup_topkg <- mc2_nodes_topkg %>%\n  group_by(id) %>%\n  summarise(count=n()) %>%\n  filter(count>1) %>%\n  left_join(mc2_edges_topkg%>%\n              group_by(from)%>%\n              summarise(from_weights= sum(weights)),\n            by=c(\"id\"=\"from\")) %>%\n  left_join(mc2_edges_topkg%>%\n              group_by(to)%>%\n              summarise(to_weights=sum(weights)),\n            by=c(\"id\"=\"to\")) %>%\n  mutate(group=ifelse(from_weights >= to_weights, \"fisher\",\"wholesaler\"))\n\nmc2_nodes_topkg <- mc2_nodes_topkg %>%\n  left_join(lookup_topkg%>%select(id,group),by=\"id\") %>%\n  mutate(group = ifelse(!is.na(group.y),group.y,group.x)) %>%\n  select(-group.y,-group.x) %>%\n  distinct()\n\n\n\nCheck mutual-exclusiveness of category.\n\n\n\nShow the code\nmc2_nodes_topcount %>%\n  group_by(id) %>%\n  summarise(count=n())%>%\n  filter(count>1)\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: id <chr>, count <int>\n\n\n\nBuild interactive network graph.\n\n\n\nShow the code\nvisNetwork(mc2_nodes_topkg,mc2_edges_topkg) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\n\nPlease mind here:\n\n\n\nHere we find the transition pattern is more of directed from one fisher to multiple wholesaler, this may due to the average weight(kg) of the product is relatively higher, so different parts of fish need to sale to different dealers."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class_Ex04",
    "section": "",
    "text": "Load packages and data:\n\npacman::p_load(rstatix, gt, patchwork, tidyverse)\n\nexam_data <-read_csv(\"data/Exam_data.csv\")\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data,\n      aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome emphasizing notice\n\n\n\nqq <- ggplot(exam_data,\n             aes(sample=ENGLISH)) +\n  stat_qq() +\n  stat_qq_line()\n\nsw_t <- exam_data %>%\n  shapiro_test(ENGLISH) %>%\n  gt()\n\ntmp <- tempfile(fileext = '.png')\ngtsave(sw_t, tmp)\ntable_png <- png::readPNG(tmp, native = TRUE)\n\nqq + table_png"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-products-services-of-each-kind-of-nodes",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-products-services-of-each-kind-of-nodes",
    "title": "Take-home_Ex03",
    "section": "3.3 Check products & services of each kind of nodes:",
    "text": "3.3 Check products & services of each kind of nodes:\n\n\nShow the code\nproduct_check <- left_join(mc3_nodes %>%\n  group_by(type) %>%\n  summarise(nodes=n()),\n  mc3_nodes %>%\n  mutate(n_0 = str_count(product_services, \"character\")) %>%\n  filter(n_0>0) %>%\n  group_by(type) %>%\n  summarise(empty_product=n()),\n  by=join_by(type),\n  keep=FALSE)%>%\n  mutate(nodes_with_product=nodes-empty_product)\n\nproduct_check\n\n\n# A tibble: 3 × 4\n  type             nodes empty_product nodes_with_product\n  <chr>            <int>         <int>              <int>\n1 Beneficial Owner 11949         11926                 23\n2 Company           8639             1               8638\n3 Company Contacts  7034          7033                  1\n\n\n\n\n\n\n\n\nCaution Here:\n\n\n\nMost of Beneficial Owner and Company Contacts nodes’ product_services are indicated as “character(0)”, so only “Company” type of nodes got specific product and services description."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-company-type-of-nodes-with-most-company-contacts-type-of-edges.",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#check-company-type-of-nodes-with-most-company-contacts-type-of-edges.",
    "title": "Take-home_Ex03",
    "section": "4.1 Check “Company” type of nodes with most “Company Contacts” type of edges.",
    "text": "4.1 Check “Company” type of nodes with most “Company Contacts” type of edges.\n\nFind those “Company Contacts” edges of “Company” nodes.\n\n\n\nShow the code\ncompany_contacts <- mc3_nodes%>%\n  select(id,type) %>%\n  filter(type=='Company') %>%\n  distinct() %>%\n  inner_join(mc3_edges%>%\n              filter(type=='Company Contacts'),\n            by=join_by(id==source),\n            keep=TRUE,\n            multiple=\"all\",\n            suffix=c('_nodes','_edges')) %>% \n  group_by(source,target) %>%\n  summarise(weight=sum(weight)) %>%\n  arrange(desc(weight)) %>%\n  ungroup()\n\n\n\nExtract and Visualize the companies with most company contacts edges.\n\n\n\nShow the code\ntop10_contacts<- pull(head(company_contacts %>%\n  group_by(source) %>%\n  summarise(count=n(),weight=sum(weight)) %>%\n  arrange(desc(count)), 10),source)\n\nggplot(data = company_contacts %>%\n  group_by(source) %>%\n  summarise(count=n(), weight=sum(weight)) %>%\n  arrange(desc(count)) %>%\n  head(10),\n       aes(x = reorder(source, weight),y=weight)) +\n  geom_col()+\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Company\",\n      y = \"Weight\",\n      title = \"Top 10 Companies with most company contacts\")\n\n\n\n\n\n\n4.1.1 Network of source “Aqua Aura SE Marine life”\n\nFilter edges by source\n\n\n\nShow the code\nedges1 <- mc3_edges %>% \n  filter(grepl(top10_contacts[1],source)) %>%\n  group_by(source,target,type) %>%\n  summarise(weight=sum(weight)) %>%\n  drop_na(weight) %>%\n  ungroup()\n\n\n\nExtract nodes.\n\n\n\nShow the code\nnodes1_extract <- rbind(edges1 %>%\n                           rename(id=source)%>%\n                             mutate(group='company') %>% \n                             select(id,group)%>%\n                             distinct(),\n                         edges1 %>%\n                           rename(id=target) %>%\n                           rename(group=type) %>%\n                           select(id,group) %>%\n                           distinct())\n\n\n\nCheck any duplication of nodes:\n\n\n\nShow the code\nnodes1_extract%>%\n  group_by(id) %>%\n  summarise(count=n())%>%\n  filter(count>1)\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: id <chr>, count <int>\n\n\n\nBuilding network model\n\n\n\nShow the code\ngraph1 <- tbl_graph(nodes = nodes1_extract,\n                       edges = edges1,\n                       directed = FALSE) %>%\n  mutate(between = centrality_betweenness(),\n         close = centrality_closeness(),\n         degree = centrality_degree())\n\n\n\n\nShow the code\ngraph1 %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(width=weight,color=type), \n                 alpha=0.8) +\n  scale_edge_width(range = c(0.5,3)) +\n  geom_node_point(aes(alpha=0.5,\n                  size = degree,\n                  color = group)) + \n  scale_size_continuous(range=c(1,5))+\n  geom_node_text(aes(filter=degree >= 3, label=id), show.legend = FALSE, size=4) +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\n\n\nHere we can see this is a normal network which connected with relatively more beneficial owner and some company contacts. Hence, the heaviest weight is 3 with majority of weight is 1.\n\n\n\n\n4.1.2 Network of “Irish Mackerel S.A.”\n\nFilter edges and then Extract nodes\n\n\n\nShow the code\nedges2 <- mc3_edges %>% \n  filter(grepl(top10_contacts[2],source)) %>%\n  group_by(source,target,type) %>%\n  summarise(weight=sum(weight)) %>%\n  drop_na(weight) %>%\n  ungroup()\n\n\n\n\nShow the code\nnodes2_extract <- rbind(edges2 %>%\n                           rename(id=source)%>%\n                             mutate(group='company') %>% \n                             select(id,group)%>%\n                             distinct(),\n                         edges2 %>%\n                           rename(id=target) %>%\n                           rename(group=type) %>%\n                           select(id,group) %>%\n                           distinct())\n\n\n\nBuilding network model\n\n\n\nShow the code\ngraph2 <- tbl_graph(nodes = nodes2_extract,\n                       edges = edges2,\n                       directed = FALSE) %>%\n  mutate(between = centrality_betweenness(),\n         close = centrality_closeness(),\n         degree = centrality_degree())\n\n\n\n\nShow the code\ngraph2 %>%\n    ggraph(layout = \"fr\") +\n  geom_edge_link(aes(width=weight,color=type)) +\n  scale_edge_width(range = c(0.5,3)) +\n  geom_node_point(aes(size = degree,\n                  color = group)) + \n  scale_size_continuous(range=c(1,5))+\n  geom_node_text(aes(filter=degree >= 3, label=id), show.legend = FALSE, size=4) +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nCaution Here:\n\n\n\nHere we can see an abnormal network in which most of edges are connected with it’s company contacts, and the edge with highest weight is connected with it’s company contracts.\n\n\n\n\n4.1.3 Network of “Kerala S.A.”\n\nFilter edges and then Extract nodes\n\n\n\nShow the code\nedges3 <- mc3_edges %>% \n  filter(grepl(top10_contacts[3],source)) %>%\n  group_by(source,target,type) %>%\n  summarise(weight=sum(weight)) %>%\n  drop_na(weight) %>%\n  ungroup()\n\n\n\n\nShow the code\nnodes3_extract <- rbind(edges3 %>%\n                           rename(id=source)%>%\n                             mutate(group='company') %>% \n                             select(id,group)%>%\n                             distinct(),\n                         edges3 %>%\n                           rename(id=target) %>%\n                           rename(group=type) %>%\n                           select(id,group) %>%\n                           distinct())\n\n\n\nBuilding network model\n\n\n\nShow the code\ngraph3 <- tbl_graph(nodes = nodes3_extract,\n                       edges = edges3,\n                       directed = FALSE) %>%\n  mutate(between = centrality_betweenness(),\n         close = centrality_closeness(),\n         degree = centrality_degree())\n\n\n\n\nShow the code\ngraph3 %>%\n    ggraph(layout = \"kk\") +\n  geom_edge_link(aes(width=weight,color=type)) +\n  scale_edge_width(range = c(0.5,3)) +\n  geom_node_point(aes(size = degree,\n                  color = group)) + \n  scale_size_continuous(range=c(1,5))+\n  geom_node_text(aes(filter=degree >= 3, label=id), show.legend = FALSE, size=4) +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nCaution Here:\n\n\n\nSimilar anomalies as previous one."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#network-of-source-aqua-aura-se-marine-life",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#network-of-source-aqua-aura-se-marine-life",
    "title": "Take-home_Ex03",
    "section": "4.1.1 Network of source “Aqua Aura SE Marine life”",
    "text": "4.1.1 Network of source “Aqua Aura SE Marine life”\n\nFilter edges by source\n\n\n\nShow the code\nedges1 <- mc3_edges %>% \n  filter(grepl(top10_contacts[1],source)) %>%\n  group_by(source,target,type) %>%\n  summarise(weight=sum(weight)) %>%\n  drop_na(weight) %>%\n  ungroup()\n\n\n\nExtract nodes.\n\n\n\nShow the code\nnodes1_extract <- rbind(edges1 %>%\n                           rename(id=source)%>%\n                             mutate(group='company') %>% \n                             select(id,group)%>%\n                             distinct(),\n                         edges1 %>%\n                           rename(id=target) %>%\n                           rename(group=type) %>%\n                           select(id,group) %>%\n                           distinct())\n\n\n\nCheck any duplication of nodes:\n\n\n\nShow the code\nnodes1_extract%>%\n  group_by(id) %>%\n  summarise(count=n())%>%\n  filter(count>1)\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: id <chr>, count <int>\n\n\n\nBuilding network model\n\n\n\nShow the code\ngraph1 <- tbl_graph(nodes = nodes1_extract,\n                       edges = edges1,\n                       directed = FALSE) %>%\n  mutate(between = centrality_betweenness(),\n         close = centrality_closeness(),\n         degree = centrality_degree())\n\n\n\n\nShow the code\ngraph1 %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(width=weight,color=type), \n                 alpha=0.8) +\n  scale_edge_width(range = c(0.5,3)) +\n  geom_node_point(aes(alpha=0.5,\n                  size = degree,\n                  color = group)) + \n  scale_size_continuous(range=c(1,5))+\n  geom_node_text(aes(filter=degree >= 3, label=id), show.legend = FALSE, size=4) +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\n\n\nHere we can see this is a normal network which connected with relatively more beneficial owner and some company contacts. Hence, the heaviest weight is 3 with majority of weight is 1.\n\n\n\n4.1.2 Network of “Irish Mackerel S.A.”\n\nFilter edges and then Extract nodes\n\n\n\nShow the code\nedges2 <- mc3_edges %>% \n  filter(grepl(top10_contacts[2],source)) %>%\n  group_by(source,target,type) %>%\n  summarise(weight=sum(weight)) %>%\n  drop_na(weight) %>%\n  ungroup()\n\n\n\n\nShow the code\nnodes2_extract <- rbind(edges2 %>%\n                           rename(id=source)%>%\n                             mutate(group='company') %>% \n                             select(id,group)%>%\n                             distinct(),\n                         edges2 %>%\n                           rename(id=target) %>%\n                           rename(group=type) %>%\n                           select(id,group) %>%\n                           distinct())\n\n\n\nBuilding network model\n\n\n\nShow the code\ngraph2 <- tbl_graph(nodes = nodes2_extract,\n                       edges = edges2,\n                       directed = FALSE) %>%\n  mutate(between = centrality_betweenness(),\n         close = centrality_closeness(),\n         degree = centrality_degree())\n\n\n\n\nShow the code\ngraph2 %>%\n    ggraph(layout = \"fr\") +\n  geom_edge_link(aes(width=weight,color=type)) +\n  scale_edge_width(range = c(0.5,3)) +\n  geom_node_point(aes(size = degree,\n                  color = group)) + \n  scale_size_continuous(range=c(1,5))+\n  geom_node_text(aes(filter=degree >= 3, label=id), show.legend = FALSE, size=4) +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nCaution Here:\n\n\n\nHere we can see an abnormal network in which most of edges are connected with it’s company contacts, and the edge with highest weight is connected with it’s company contracts.\n\n\n\n\n4.1.3 Network of “Kerala S.A.”\n\nFilter edges and then Extract nodes\n\n\n\nShow the code\nedges3 <- mc3_edges %>% \n  filter(grepl(top10_contacts[3],source)) %>%\n  group_by(source,target,type) %>%\n  summarise(weight=sum(weight)) %>%\n  drop_na(weight) %>%\n  ungroup()\n\n\n\n\nShow the code\nnodes3_extract <- rbind(edges3 %>%\n                           rename(id=source)%>%\n                             mutate(group='company') %>% \n                             select(id,group)%>%\n                             distinct(),\n                         edges3 %>%\n                           rename(id=target) %>%\n                           rename(group=type) %>%\n                           select(id,group) %>%\n                           distinct())\n\n\n\nBuilding network model\n\n\n\nShow the code\ngraph3 <- tbl_graph(nodes = nodes3_extract,\n                       edges = edges3,\n                       directed = FALSE) %>%\n  mutate(between = centrality_betweenness(),\n         close = centrality_closeness(),\n         degree = centrality_degree())\n\n\n\n\nShow the code\ngraph3 %>%\n    ggraph(layout = \"kk\") +\n  geom_edge_link(aes(width=weight,color=type)) +\n  scale_edge_width(range = c(0.5,3)) +\n  geom_node_point(aes(size = degree,\n                  color = group)) + \n  scale_size_continuous(range=c(1,5))+\n  geom_node_text(aes(filter=degree >= 3, label=id), show.legend = FALSE, size=4) +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n\nCaution Here:\n\n\n\nSimilar anomalies as previous one."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#text-sensing-and-nodes-categorization-for-company-id",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#text-sensing-and-nodes-categorization-for-company-id",
    "title": "Take-home_Ex03",
    "section": "4.2 Text sensing and nodes categorization for company ID:",
    "text": "4.2 Text sensing and nodes categorization for company ID:\n\n4.2.1 Extract id text and preprocessing.\n\nWord tokenization with punctuation and no lowercasing\n\n\n\nShow the code\ntidy_nodes <- mc3_nodes %>%\n  unnest_tokens(word, id, to_lower = TRUE, strip_punct = TRUE,drop = FALSE)\n\n\n\nCustomize stop words and Remove stop words\n\n\n\nShow the code\n# Create a list of custom stopwords that should be added\nword <- c(\"llc\",\"plc\",\"ltd\",\"inc\",\"de\",\"del\",\"company\",\"corporation\",\"liability\")\nlexicon <-  rep(\"custom\", times=length(word))\n\n# Create a dataframe from the two vectors above\nmystopwords <- data.frame(word, lexicon)\nnames(mystopwords) <- c(\"word\", \"lexicon\")\n\n# Add the dataframe to stop_words df that exists in the library stopwords\nstop_words <-  dplyr::bind_rows(stop_words, mystopwords)\n\n\n\n\nShow the code\nstopwords_removed <- tidy_nodes %>% \n  anti_join(stop_words) \n\n\n\n\n\n\n\n\nNotes:\n\n\n\nHere need to customize stop words to include some company suffixes.\n\n\n\n\n4.2.2 Visualize the id words with most counts\n\n\nShow the code\nstopwords_removed %>%\n  count(word, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(x = word, y = n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Unique words of ID\",\n      y = \"Count\",\n      title = \"Top 15 Count of unique words found in company ID\")\n\n\n\n\n\n\n\n4.2.3 Check “company” ID highest frequency word:\n\nFilter nodes with “Sons” in their ID text:\n\n\n\nShow the code\nnodes_sons <- mc3_nodes %>%\n  filter(type=='Company') %>%\n  select(\"id\",\"country\",\"type\",\"product_services\") %>%\n  mutate(n_sons = str_count(id, \"Sons\")) %>%\n  filter(n_sons>0) %>%\n  distinct()\n\n\n\n\n\n\n\n\nNotes:\n\n\n\nHere I only took out the “company” type of nodes with “Sons” in their ID.\n\n\n\n\n4.2.4 Network of “Sons”s\n\nFilter edges by those ID we found with “Sons” and then Extract nodes\n\n\n\nShow the code\nedges_sons <- nodes_sons%>%\n  select(id,country) %>%\n  distinct() %>%\n  left_join(mc3_edges,by=join_by(id==source),keep=TRUE,multiple=\"all\") %>% \n  group_by(source,target,type) %>%\n  summarise(weight=sum(weight)) %>%\n  drop_na(weight) %>%\n  ungroup()\n\n\n\n\nShow the code\nnodes_sons_extract <- rbind(edges_sons %>%\n                           rename(id=source)%>%\n                             mutate(group='company') %>% \n                             select(id,group)%>%\n                             distinct(),\n                         edges_sons %>%\n                           rename(id=target) %>%\n                           rename(group=type) %>%\n                           select(id,group) %>%\n                           distinct()) %>% \n  left_join(mc3_nodes%>%select(\"id\"), \n             by = 'id', unmatched=\"drop\", keep = FALSE, multiple= 'first')\n\n\n\nBuilding network model\n\n\n\nShow the code\nvisNetwork(nodes_sons_extract,edges_sons%>%\n             rename(from=source)%>%\n             rename(to=target)) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion:\n\n\n\nThose nodes are individually connected with different set of nodes, indicating there’s no intensely close connected group of nodes."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#nodes-categorization-by-company-id",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_Ex03.html#nodes-categorization-by-company-id",
    "title": "Take-home_Ex03",
    "section": "4.2 Nodes categorization by company ID:",
    "text": "4.2 Nodes categorization by company ID:\n\n4.2.1 Extract id text and preprocessing.\n\nWord tokenization with punctuation and no lowercasing\n\n\n\nShow the code\ntidy_nodes <- mc3_nodes %>%\n  unnest_tokens(word, id, to_lower = TRUE, strip_punct = TRUE,drop = FALSE)\n\n\n\nCustomize stop words and Remove stop words\n\n\n\nShow the code\n# Create a list of custom stopwords that should be added\nword <- c(\"llc\",\"plc\",\"ltd\",\"inc\",\"de\",\"del\",\"company\",\"corporation\",\"liability\")\nlexicon <-  rep(\"custom\", times=length(word))\n\n# Create a dataframe from the two vectors above\nmystopwords <- data.frame(word, lexicon)\nnames(mystopwords) <- c(\"word\", \"lexicon\")\n\n# Add the dataframe to stop_words df that exists in the library stopwords\nstop_words <-  dplyr::bind_rows(stop_words, mystopwords)\n\n\n\n\nShow the code\nstopwords_removed <- tidy_nodes %>% \n  anti_join(stop_words) \n\n\n\n\n\n\n\n\nNotes:\n\n\n\nHere need to customize stop words to include some company suffixes.\n\n\n\n\n4.2.2 Visualize the id words with most counts\n\n\nShow the code\nstopwords_removed %>%\n  count(word, sort = TRUE) %>%\n  top_n(15) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(x = word, y = n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip() +\n      labs(x = \"Unique words of ID\",\n      y = \"Count\",\n      title = \"Top 15 Count of unique words found in company ID\")\n\n\n\n\n\n\n\n4.2.3 Check “company” ID highest frequency word:\n\nFilter nodes with “Sons” in their ID text:\n\n\n\nShow the code\nnodes_sons <- mc3_nodes %>%\n  filter(type=='Company') %>%\n  select(\"id\",\"country\",\"type\",\"product_services\") %>%\n  mutate(n_sons = str_count(id, \"Sons\")) %>%\n  filter(n_sons>0) %>%\n  distinct()\n\n\n\n\n\n\n\n\nNotes:\n\n\n\nHere I only took out the “company” type of nodes with “Sons” in their ID.\n\n\n\nFilter edges by those ID we found with “Sons” and then Extract nodes\n\n\n\nShow the code\nedges_sons <- nodes_sons%>%\n  select(id,country) %>%\n  distinct() %>%\n  left_join(mc3_edges,by=join_by(id==source),keep=TRUE,multiple=\"all\") %>% \n  group_by(source,target,type) %>%\n  summarise(weight=sum(weight)) %>%\n  drop_na(weight) %>%\n  ungroup()\n\n\n\n\nShow the code\nnodes_sons_extract <- rbind(edges_sons %>%\n                           rename(id=source)%>%\n                             mutate(group='company') %>% \n                             select(id,group)%>%\n                             distinct(),\n                         edges_sons %>%\n                           rename(id=target) %>%\n                           rename(group=type) %>%\n                           select(id,group) %>%\n                           distinct()) %>% \n  left_join(mc3_nodes%>%select(\"id\"), \n             by = 'id', unmatched=\"drop\", keep = FALSE, multiple= 'first')\n\n\n\nBuilding network model\n\n\n\nShow the code\nvisNetwork(nodes_sons_extract,edges_sons%>%\n             rename(from=source)%>%\n             rename(to=target)) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\n\n\nThose nodes are individually connected with different set of nodes, and no obvious cluster of company contacts, indicating there’s no intensely close connected group of companies.\n\n\n\n\n4.2.4 Check “company” ID 2nd highest frequency word:\n\nFilter nodes with “Smith” in their ID text:\n\n\n\nShow the code\nnodes_smith <- mc3_nodes %>%\n  filter(type=='Company') %>%\n  select(\"id\",\"country\",\"type\",\"product_services\") %>%\n  mutate(n_sons = str_count(id, \"Smith\")) %>%\n  filter(n_sons>0) %>%\n  distinct()\n\n\n\nFilter edges by those ID we found and then Extract nodes\n\n\n\nShow the code\nedges_smith <- nodes_smith%>%\n  select(id,country) %>%\n  distinct() %>%\n  left_join(mc3_edges,by=join_by(id==source),keep=TRUE,multiple=\"all\") %>% \n  group_by(source,target,type) %>%\n  summarise(weight=sum(weight)) %>%\n  drop_na(weight) %>%\n  ungroup()\n\n\n\n\nShow the code\nnodes_smith_extract <- rbind(edges_smith %>%\n                           rename(id=source)%>%\n                             mutate(group='company') %>% \n                             select(id,group)%>%\n                             distinct(),\n                         edges_smith %>%\n                           rename(id=target) %>%\n                           rename(group=type) %>%\n                           select(id,group) %>%\n                           distinct()) %>% \n  left_join(mc3_nodes%>%select(\"id\"), \n             by = 'id', unmatched=\"drop\", keep = FALSE, multiple= 'first')\n\n\n\nBuilding network model\n\n\n\nShow the code\nvisNetwork(nodes_smith_extract,edges_smith%>%\n             rename(from=source)%>%\n             rename(to=target)) %>%\n  visIgraphLayout(layout = \"layout_with_fr\") %>%\n  visEdges(smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %>%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %>%\n  visLegend() %>%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\n\n\n\n\n\nNotes:\n\n\n\nSimilar pattern with previous network with several central nodes which are connected with a set of beneficial owners and company contacts."
  }
]